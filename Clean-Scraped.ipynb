{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in raw scraped data corresponding to the 3 volumes\n",
    "df1 = pd.read_csv('scraped/vol_1_all.csv')\n",
    "df2 = pd.read_csv('scraped/vol_2_all.csv')\n",
    "df3 = pd.read_csv('scraped/vol_3_all.csv')\n",
    "# Some letters were missed in initial scrape, so we fill them in here\n",
    "df2_missing = pd.read_csv('scraped/vol_2_missing.csv')\n",
    "df3_missing = pd.read_csv('scraped/vol_3_missing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Letter Number from Raw HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_letter_number(html):\n",
    "    '''Given HTML corresponding to a scraped letter, extract the letter number'''\n",
    "    soup = BeautifulSoup(html, 'html.parser', from_encoding='utf-8')\n",
    "    \n",
    "    # Remove page breaks if they exist\n",
    "    if soup.find('span', class_=\"page-break\"):\n",
    "        soup.find('span', class_=\"page-break\").decompose()\n",
    "    \n",
    "    return soup.text.split('.')[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"bs4.dammit\"\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty dataframe for storing html with corresponding volume and letter numbers\n",
    "labelled_df = pd.DataFrame()\n",
    "\n",
    "for df in [df1, df2, df3, df2_missing, df3_missing]:\n",
    "    if df is df1:\n",
    "        vol = 1\n",
    "    elif df is df2 or df is df2_missing:\n",
    "        vol = 2\n",
    "    else:\n",
    "        vol = 3\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        html = row['content']\n",
    "        # Extract letter number from raw html\n",
    "        letter_no = extract_letter_number(html)\n",
    "        \n",
    "        # Append to dataframe only if letter number successfully extracted\n",
    "        if letter_no.isdigit():\n",
    "            labelled_df = labelled_df.append({\n",
    "                        'Vol': vol,\n",
    "                        'LetterNo': int(letter_no),\n",
    "                        'raw_html': html,\n",
    "                        }, \n",
    "                        ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sort by volume, then letter number\n",
    "labelled_df = labelled_df.sort_values(['Vol', 'LetterNo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove non-ASCII Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_non_ascii(text):\n",
    "    '''Remove non-ASCII characters'''\n",
    "    if text is not np.nan:\n",
    "        return ''.join(i for i in text if ord(i)<128)\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract text from Raw HTML\n",
    "Removing:\n",
    "* Letter headings\n",
    "* Footnote numbers (span class=\"fragment-marker work-legend\" data-type=\"fragment-marker\")\n",
    "* Line breaks (class = \"page-break\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_text(html):\n",
    "    '''Given HTML corresponding to a scraped letter, extract the letter number'''\n",
    "    soup = BeautifulSoup(html, 'html.parser', from_encoding='utf-8')\n",
    "    \n",
    "    # Remove page breaks if they exist\n",
    "    for pgbreak in soup.findAll('span', class_=\"page-break\"):\n",
    "        pgbreak.decompose()\n",
    "        \n",
    "    # Remove footnote hyperlinks if they exist\n",
    "    for footnote in soup.findAll('span', class_=\"fragment-marker work-legend\"):\n",
    "        footnote.decompose()\n",
    "        \n",
    "    # Remove header (letter number and title)\n",
    "    if soup.find('h2'):\n",
    "        soup.find('h2').decompose()\n",
    "    \n",
    "    if soup.find('h3'):\n",
    "        soup.find('h3').decompose()\n",
    "        \n",
    "    if soup.find('h4'):\n",
    "        soup.find('h4').decompose()\n",
    "    \n",
    "    return soup.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelled_df['text'] = labelled_df['raw_html'].apply(extract_text)\\\n",
    "                                            .apply(remove_non_ascii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop empty rows and any duplicates\n",
    "labelled_df = labelled_df.dropna()\n",
    "labelled_df = labelled_df[~labelled_df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelled_df.to_csv('csv/letters.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge with Metadata Dataframe based on Volume and Letter Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read cleaned dataframes\n",
    "text_df = pd.read_csv('csv/letters.csv')\n",
    "meta_df = pd.read_csv('csv/032818_RAC_Networks_Database.csv')\n",
    "\n",
    "# Join dataframes on Letter Number and Volume\n",
    "merged_df = pd.merge(meta_df, text_df, on=['Vol', 'LetterNo'])\n",
    "\n",
    "# Filter out for a subset of the columns\n",
    "merged_filtered = merged_df[[u'UID', u'Vol', u'LetterNo', u'Sender',  u'Place Sent From', u'Ship Name', u'Place Going To', u'Date',\n",
    "          u'Boat/Fort', u'RAC/Other Nation', u'text', u'Year', u'Month']].copy()\n",
    "\n",
    "# Remove non-ASCII\n",
    "for col in merged_filtered.columns:\n",
    "    if col in [u'Sender',u'Place Sent From',u'Ship Name', u'Place Going To',u'Date',u'Boat/Fort', u'RAC/Other Nation', u'text']:\n",
    "        merged_filtered[col] = merged_filtered[col].apply(remove_non_ascii)\n",
    "        \n",
    "# Replace 1000 years with None\n",
    "merged_filtered['Year'] = merged_filtered['Year'].replace(1000, np.NaN)\n",
    "\n",
    "# Export joined dataframe to csv file\n",
    "merged_filtered.to_csv('csv/metadata_text_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning\n",
    "From https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/\n",
    "\n",
    "- lowercasing\n",
    "- punctuation removal\n",
    "- removal of stopwords\n",
    "- Deduping of aliases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Cleaning Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_aliases(text, alias_to_name, aliases):\n",
    "    for alias in aliases:\n",
    "        if alias in text:\n",
    "            text = text.replace(alias, alias_to_name[alias])\n",
    "    \n",
    "    return text\n",
    "\n",
    "def correct_common_mispellings(text):\n",
    "    vowels = ['a', 'e', 'i', 'o', 'u']\n",
    "    # Words not to change with the i before e rule\n",
    "    ie_exceptions = ['specie', 'species', 'science', 'sufficient', 'seize', 'vein', 'weird', 'their', 'feisty', 'foreign', 'eight', 'neighbor', 'neighbors', '']\n",
    "    text_split = text.split(' ')\n",
    "    \n",
    "    for i, word in enumerate(text_split):\n",
    "        # If word starts with ff -> f\n",
    "        if word.startswith('ff'):\n",
    "            text_split[i] = text_split[i][1:]\n",
    "        # If word ends with tt -> t\n",
    "        if word.endswith('tt'):\n",
    "            text_split[i] = text_split[i][:-1]\n",
    "        # If word ends with pp -> p\n",
    "        if word.endswith('pp'):\n",
    "            text_split[i] = text_split[i][:-1]\n",
    "        # Replace comeing with coming\n",
    "        if word == 'comeing':\n",
    "            text_split[i] = 'coming'\n",
    "        # ei corrected to ie\n",
    "#         if 'ei' in text_split[i] and not (text_split[i].startswith('ei') or text_split[i].endswith('ei')):\n",
    "#             text_split[i] = word.replace('ei', 'ie')\n",
    "        # TODO: receive, perceive, deceive, eight\n",
    "        # Implement i before e except for c|\n",
    "        # Remove final \"e\" after two consonants\n",
    "        if word.endswith('e') and len(word) > 2 and word[-2] not in vowels and word[-3] not in vowels:\n",
    "            text_split[i] = text_split[i][:-1]\n",
    "            \n",
    "    return ' '.join(text_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load aliases that we want to dedupe in the text\n",
    "alias_df = pd.DataFrame()\n",
    "\n",
    "for alias_file in os.listdir('aliases/'):\n",
    "    temp = pd.read_csv('aliases/' + alias_file, header=None)\n",
    "    # Append to alias_df\n",
    "    alias_df = alias_df.append(temp)\n",
    "\n",
    "# Regex for alphabetical characters\n",
    "alpha_regex = re.compile('[^a-zA-Z\\s]')\n",
    "\n",
    "# Dict mapping alias to name\n",
    "alias_to_name = {}\n",
    "\n",
    "for _, row in alias_df.iterrows():\n",
    "    name = alpha_regex.sub('', row[0].strip().lower())\n",
    "\n",
    "    if row[1] is not np.nan and row[1] != '':\n",
    "        aliases = [alpha_regex.sub('', s.strip().lower()) for s in row[1].split(';')]\n",
    "    \n",
    "    for alias in aliases:\n",
    "        if alias != '':\n",
    "            alias_to_name[alias] = name\n",
    "\n",
    "# Sort aliases in decreasing order by length so that longer phrases are replaced first\n",
    "aliases = alias_to_name.keys()\n",
    "aliases.sort(key=len, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load joined dataframe\n",
    "df = pd.read_csv('csv/metadata_text_merged.csv')\n",
    "# Remove rows with empty text column for now\n",
    "df = df.dropna(subset=['text'])\n",
    "# Filter out Barbados texts for our analysis\n",
    "df = df[df['Place Sent From'] != 'Barbados']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lowercasing\n",
    "df['text_cleaned'] = df['text'].apply((lambda x: \" \".join(x.lower() for x in x.split())))\n",
    "# remove puctuation\n",
    "df['text_cleaned'] = df['text_cleaned'].str.replace('[^\\w\\s]','')\n",
    "# stopword removal\n",
    "stop = stopwords.words('english')\n",
    "df['text_cleaned'] = df['text_cleaned'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "# Replace all aliases with the term they actually refer to \n",
    "df['text_cleaned_dealiased'] = df['text_cleaned'].apply(lambda x: replace_aliases(x, alias_to_name, aliases))\n",
    "# Correct common mispellings\n",
    "df['text_cleaned_dealiased'] = df['text_cleaned_dealiased'].apply(lambda x: correct_common_mispellings(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save processed dataframe\n",
    "df.to_csv('csv/metadata_text_merged_cleaned.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
