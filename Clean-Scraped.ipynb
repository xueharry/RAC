{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read in raw scraped data corresponding to the 3 volumes\n",
    "df1 = pd.read_csv('scraped/vol_1_all.csv')\n",
    "df2 = pd.read_csv('scraped/vol_2_all.csv')\n",
    "df3 = pd.read_csv('scraped/vol_3_all.csv')\n",
    "# Some letters were missed in initial scrape, so we fill them in here\n",
    "df2_missing = pd.read_csv('scraped/vol_2_missing.csv')\n",
    "df3_missing = pd.read_csv('scraped/vol_3_missing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Letter Number from Raw HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_letter_number(html):\n",
    "    '''Given HTML corresponding to a scraped letter, extract the letter number'''\n",
    "    soup = BeautifulSoup(html, 'html.parser', from_encoding='utf-8')\n",
    "    \n",
    "    # Remove page breaks if they exist\n",
    "    if soup.find('span', class_=\"page-break\"):\n",
    "        soup.find('span', class_=\"page-break\").decompose()\n",
    "    \n",
    "    return soup.text.split('.')[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handlers could be found for logger \"bs4.dammit\"\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty dataframe for storing html with corresponding volume and letter numbers\n",
    "labelled_df = pd.DataFrame()\n",
    "\n",
    "for df in [df1, df2, df3, df2_missing, df3_missing]:\n",
    "    if df is df1:\n",
    "        vol = 1\n",
    "    elif df is df2 or df is df2_missing:\n",
    "        vol = 2\n",
    "    else:\n",
    "        vol = 3\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        html = row['content']\n",
    "        # Extract letter number from raw html\n",
    "        letter_no = extract_letter_number(html)\n",
    "        \n",
    "        # Append to dataframe only if letter number successfully extracted\n",
    "        if letter_no.isdigit():\n",
    "            labelled_df = labelled_df.append({\n",
    "                        'Vol': vol,\n",
    "                        'LetterNo': int(letter_no),\n",
    "                        'raw_html': html,\n",
    "                        }, \n",
    "                        ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sort by volume, then letter number\n",
    "labelled_df = labelled_df.sort_values(['Vol', 'LetterNo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove non-ASCII Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_non_ascii(text):\n",
    "    '''Remove non-ASCII characters'''\n",
    "    if text is not np.nan:\n",
    "        return ''.join(i for i in text if ord(i)<128)\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract text from Raw HTML\n",
    "Removing:\n",
    "* Letter headings\n",
    "* Footnote numbers (span class=\"fragment-marker work-legend\" data-type=\"fragment-marker\")\n",
    "* Line breaks (class = \"page-break\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_text(html):\n",
    "    '''Given HTML corresponding to a scraped letter, extract the letter number'''\n",
    "    soup = BeautifulSoup(html, 'html.parser', from_encoding='utf-8')\n",
    "    \n",
    "    # Remove page breaks if they exist\n",
    "    for pgbreak in soup.findAll('span', class_=\"page-break\"):\n",
    "        pgbreak.decompose()\n",
    "        \n",
    "    # Remove footnote hyperlinks if they exist\n",
    "    for footnote in soup.findAll('span', class_=\"fragment-marker work-legend\"):\n",
    "        footnote.decompose()\n",
    "        \n",
    "    # Remove header (letter number and title)\n",
    "    if soup.find('h2'):\n",
    "        soup.find('h2').decompose()\n",
    "    \n",
    "    if soup.find('h3'):\n",
    "        soup.find('h3').decompose()\n",
    "        \n",
    "    if soup.find('h4'):\n",
    "        soup.find('h4').decompose()\n",
    "    \n",
    "    return soup.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelled_df['text'] = labelled_df['raw_html'].apply(extract_text)\\\n",
    "                                            .apply(remove_non_ascii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop empty rows and any duplicates\n",
    "labelled_df = labelled_df.dropna()\n",
    "labelled_df = labelled_df[~labelled_df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelled_df.to_csv('csv/letters.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge with Metadata Dataframe based on Volume and Letter Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read cleaned dataframes\n",
    "text_df = pd.read_csv('csv/letters.csv')\n",
    "meta_df = pd.read_excel('csv/032818_RAC_Networks_Database.xlsx')\n",
    "\n",
    "# Join dataframes on Letter Number and Volume\n",
    "merged_df = pd.merge(meta_df, text_df, on=['Vol', 'LetterNo'])\n",
    "\n",
    "# Filter out for a subset of the columns\n",
    "merged_filtered = merged_df[[u'UID', u'Vol', u'LetterNo', u'Sender',  u'Place Sent From', u'Ship Name', u'Place Going To', u'Date',\n",
    "          u'Boat/Fort', u'RAC/Other Nation', u'text']].copy()\n",
    "\n",
    "# Remove non-ASCII\n",
    "for col in merged_filtered.columns:\n",
    "    if col in [u'Sender',u'Place Sent From',u'Ship Name', u'Place Going To',u'Date',u'Boat/Fort', u'RAC/Other Nation', u'text']:\n",
    "        merged_filtered[col] = merged_filtered[col].apply(remove_non_ascii)\n",
    "\n",
    "# Export joined dataframe to csv file\n",
    "merged_filtered.to_csv('csv/metadata_text_merged.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
