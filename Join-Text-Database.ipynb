{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load cleaned text and metadata datasets\n",
    "text_df = pd.read_csv('csv/segmented_cleaned.csv')\n",
    "meta_df = pd.read_excel('csv/032818_RAC_Networks_Database.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Volume and Letter Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_lookup = {}\n",
    "\n",
    "# Create lookup table mapping filename to volume number\n",
    "for volume_dir in os.listdir('First Law Scans Cropped/'):\n",
    "    # Make sure we don't iterate over .DS_Store\n",
    "    if volume_dir != '.DS_Store':\n",
    "        if 'III' in volume_dir:\n",
    "            for filename in os.listdir('First Law Scans Cropped/' + volume_dir):\n",
    "                if filename != '.DS_Store':\n",
    "                    vol_lookup[filename] = 3\n",
    "        elif 'II' in volume_dir:\n",
    "            for filename in os.listdir('First Law Scans Cropped/' + volume_dir):\n",
    "                if filename != '.DS_Store':\n",
    "                    vol_lookup[filename] = 2\n",
    "        elif 'I' in volume_dir:\n",
    "            for filename in os.listdir('First Law Scans Cropped/' + volume_dir):\n",
    "                if filename != '.DS_Store':\n",
    "                    vol_lookup[filename] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_vol(filename):\n",
    "    '''Extract Vol for each text'''\n",
    "    return vol_lookup[filename]\n",
    "\n",
    "def extract_letter_number(section_title):\n",
    "    '''Extract letter number from section title'''\n",
    "    return section_title.split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert(sum(text_df['filename'].apply(extract_vol).isnull() == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add columns for Vol and LetterNo \n",
    "text_df['Vol'] = text_df['filename'].apply(extract_vol)\n",
    "text_df['LetterNo'] = text_df['section_title'].apply(extract_letter_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_df.to_csv('csv/segmented_with_numbers.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join with Metadata dataframe on Volume and Letter Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read cleaned dataframes\n",
    "text_df = pd.read_csv('csv/segmented_with_numbers.csv')\n",
    "meta_df = pd.read_excel('csv/032818_RAC_Networks_Database.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Join dataframes on Letter Number and Volume\n",
    "merged_df = pd.merge(meta_df, text_df, on=['Vol', 'LetterNo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter out for a subset of the columns\n",
    "merged_filtered = merged_df[[u'UID', u'Vol', u'LetterNo', u'Sender',  u'Place Sent From', u'Ship Name', u'Place Going To', u'Date',\n",
    "          u'Boat/Fort', u'RAC/Other Nation', u'text', u'section_title', u'filename']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_non_ascii(text):\n",
    "    '''Remove non-ASCII characters (mostly due to OCR parsing error)'''\n",
    "    if text is not np.nan:\n",
    "        return ''.join(i for i in text if ord(i)<128)\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "# Strip away non-ASCII characters from columns\n",
    "for col in merged_filtered.columns:\n",
    "    if merged_filtered[col].dtype == np.object and col != 'Date':\n",
    "        merged_filtered[col] = merged_filtered[col].apply(remove_non_ascii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Export joined dataframe to csv file\n",
    "merged_filtered.to_csv('csv/metadata_text_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify UIDs with duplicates for manual cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "UID_counter = Counter(merged_filtered['UID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "6\n",
      "7\n",
      "13\n",
      "36\n",
      "45\n",
      "193\n",
      "244\n",
      "347\n",
      "574\n",
      "575\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "1641\n",
      "1647\n",
      "1655\n",
      "1688\n",
      "2860\n",
      "2861\n",
      "3019\n",
      "3020\n"
     ]
    }
   ],
   "source": [
    "for uid, count in UID_counter.items():\n",
    "    if count > 1:\n",
    "        print uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned merged file with manually deduped UIDs\n",
    "merged_deduped = pd.read_csv('csv/metadata_text_merged_uid_dedup.csv')\n",
    "\n",
    "UID_counter = Counter(merged_deduped['UID'])\n",
    "\n",
    "# Check to see there's no duplicates\n",
    "for uid, count in UID_counter.items():\n",
    "    if count > 1:\n",
    "        print uid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
